#SHERPA - working with pages of 100 returns

#------- STAGE 1: SHERPA API ------------------

# Load packages
library(httr)
# Jsonlite documentation: https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf
library(jsonlite)

# Set number of times to query api here (if limit = 100 then this needs to be total number of journals / 100. I ran 380 times but final 70 were empty since it skips missing IDs)
max_api_calls <- 310

# Add API key/ access token - this is Tom Kenny's token but easy to generate your own
api_key <- "E46056F4-F72C-11EA-B80C-3822122D6054"

# Creating the API url (max limit = 100)
base_api_url <- "https://v2.sherpa.ac.uk/cgi/retrieve?item-type=publication&format=Json"
limit <- 100
api_url <- paste0(base_api_url, "&api-key=", api_key, "&limit=", limit, "&offset=")

# Creating a loop/ paging requests to overcome API limits, and flattening the JSON (see https://rdrr.io/cran/jsonlite/f/vignettes/json-paging.Rmd)
pages <- list()
for(i in 1:max_api_calls){
  offset <- limit*(i-1)
  page <- fromJSON(paste0(api_url, offset), flatten = TRUE)
  page <- page$items
  message("Querying api for ", limit, " publications at offset ", offset)
  pages[[i]] <- page
}

# Combining queries into a single JSON file
message("Combining queries")
publications <- rbind_pages(pages)
save(publications, file = "sherpa_publications")
# At this point I have an R object which is combined version of all the flattened JSON pages (publications)

#-------STAGE 2: FILTERING--------------

# SHERPA filtering - start with flattened JSON list pages created in other code. SHOULD IT BE PUBLICATIONS RATHER THAN PAGE?

# Load packages
library(httr)
# Jsonlite documentation: https://cran.r-project.org/web/packages/jsonlite/jsonlite.pdf
library(jsonlite)
library(tidyverse)

# set up page as one record
page <- publications[1,]

# initialise first record

title <- unnest(page, 'title') %>% select(title) %>% head(1)

issn <- as.data.frame(page$issns) %>% select(type, issn) %>% pivot_wider(names_prefix='issn_', names_from = type, values_from = issn)

doaj <- select(page, 'listed_in_doaj')
sherpa_id <- select(page, 'id')

oa_prohibited <- as.data.frame(page$publisher_policy) %>% select(open_access_prohibited) %>% mutate(pubpol_id = seq_len(n()), title=title$title)

publisher <- unnest(as.data.frame(page$publishers), 'publisher.name') %>% select(name)

if ('permitted_oa' %in% names(as.data.frame(page$publisher_policy))){
  pubpol_list <- list()
  for (k in 1:nrow(oa_prohibited)){
    
    pubpol_temp <- unnest(as.data.frame(page$publisher_policy)[k, ], 'permitted_oa') %>%
      select(any_of(c('rowname', 'additional_oa_fee', 'article_version', 'additional_oa_fee', 'embargo.amount', 'license', 'copyright_owner'))) %>%
      mutate(pubpol_id = k)
    
    pubpol_list[[k]] <- pubpol_temp
    
  }
  
  pubpol <- do.call(bind_rows, pubpol_list)
  
  pubpol_oa <- merge(oa_prohibited, pubpol, by='pubpol_id')
  
} else {
    pubpol_oa <- oa_prohibited 
  
}

record <- merge(cbind(sherpa_id, title, issn, doaj, publisher), pubpol_oa, by='title')
records <- record

# Run loop-----------------------------------
# There are two main errors I can't get around and they both relate to the same issue - NULL values in the data. My solution so far has been to create if loops that skip the journal if it has a NULL value but obviously this isn't ideal as we're missing data
exclude <- c(1404, 2502, 4472, 7187, 8607, 8747, 9364, 13815, 16598, 16601, 17183, 17188, 17203, 17767, 17768, 18248, 18411, 20247, 21290, 21775, 21909, 22066, 22071, 22072, 22075, 22076, 22077, 22078, 22079, 22392, 22393, 22395, 22396, 22397, 22398, 22431, 22482, 22503, 22560, 22773, 22792, 25930)

# ERROR 1, ROWS = 7187, 8607, 8747, 13815, 16598, 16601, 17183, 17188, 17203, 17767, 17768, 18248, 18411, 20247, 21290, 21775, 21909, 22066, 22071, 22072, 22075, 22076, 22077, 22078, 22079, 22392, 22393, 22395,22396, 22397, 22398, 22431, 22482, 22503 - Error: Can't subset columns that don't exist. x Column `type` doesn't exist.
# ERROR 2, ROWS = 25930, 26501 - Error: Can't subset columns that don't exist. x Column `open_access_prohibited` doesn't exist. NB I HAVENT SET UP THE IF SKIP FOR THIS YET
# ERROR 4, ROW = 9364 - Error: Can't combine `..1$issn_print` <character> and `..2$issn_print` <list>.

for (i in (26520:30969)[-exclude]) {
  
  page <- publications[i,]
  
  title <- unnest(page, 'title') %>% select(title) %>% head(1)
  
  issn <- as.data.frame(page$issns)
  if(is.null(issn$type)) {next
    } else {
  issn <- as.data.frame(page$issns) %>% select(type, issn) %>% pivot_wider(names_prefix='issn_', names_from = type, values_from = issn)
    }
  
  doaj <- select(page, 'listed_in_doaj')
  
  sherpa_id <- select(page, 'id')
  
  oa_prohibited <- as.data.frame(page$publisher_policy) %>% select(open_access_prohibited) %>% mutate(pubpol_id = seq_len(n()), title=title$title)
  
  publisher <- unnest(as.data.frame(page$publishers), 'publisher.name') %>% select(name)
  
  
  if ('permitted_oa' %in% names(as.data.frame(page$publisher_policy))){
    pubpol_list <- list()
    for (k in 1:nrow(oa_prohibited)){
      
      pubpol_temp <- unnest(as.data.frame(page$publisher_policy)[k, ], 'permitted_oa') %>%
        select(any_of(c('rowname', 'additional_oa_fee', 'article_version', 'additional_oa_fee', 'embargo.amount', 'license', 'copyright_owner'))) %>%
        mutate(pubpol_id = k)
      
      pubpol_list[[k]] <- pubpol_temp
      
    }
    
    pubpol <- do.call(bind_rows, pubpol_list)
    
    pubpol_oa <- merge(oa_prohibited, pubpol, by='pubpol_id')
    
  } else {
    pubpol_oa <- oa_prohibited 
    
  }
  
  
  
  
  record <- merge(cbind(sherpa_id, title, issn, doaj, publisher), pubpol_oa, by='title')
  records <- bind_rows(records, record)
  
  message(i)

}

# ----
# write to excel

# openxlsx::write.xlsx(as.data.frame(records), 'sherpa_all_policies.xlsx')
